{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qFo4btZZ6Un",
        "outputId": "07da10a1-4647-44de-b2a2-bdcfd5e0d113"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets==2.14.6\n",
        "# !pip install transformers\n",
        "# !pip install evaluate\n",
        "# !pip install --no-cache-dir transformers sentencepiece\n",
        "# !pip install accelerate -U\n",
        "# !pip install protobuf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "nZivLybaZ-gV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import Dataset, DatasetDict\n",
        "from datasets import concatenate_datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from dataclasses import dataclass\n",
        "from transformers import AutoTokenizer, AutoModelForMultipleChoice, get_scheduler, TrainingArguments, Trainer\n",
        "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
        "from typing import Optional, Union\n",
        "import evaluate\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import accelerate\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "czFjm3I4aCYd"
      },
      "outputs": [],
      "source": [
        "train_data = np.load('./data/SP-train.npy', allow_pickle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data_lists(data):\n",
        "    questions = []\n",
        "    choices = []\n",
        "    labels = []\n",
        "    for example in data:\n",
        "        # print(example.keys())\n",
        "        questions.append(example['question'])\n",
        "        choices.append(example['choice_list'])\n",
        "        labels.append(example['label'])\n",
        "    return questions, choices, labels\n",
        "\n",
        "class SentenceModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SentenceModel, self).__init__()\n",
        "        self.fc_1 = nn.Linear(768, 256)\n",
        "        self.bn_1 = nn.BatchNorm1d(256)  \n",
        "        self.dropout_1 = nn.Dropout(0.2)  \n",
        "        self.fc_2 = nn.Linear(256, 32)\n",
        "        self.bn_2 = nn.BatchNorm1d(32)  \n",
        "        self.dropout_2 = nn.Dropout(0.3)  \n",
        "        self.fc_3 = nn.Linear(32, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc_1(x)\n",
        "        out = self.bn_1(out)  \n",
        "        out = nn.functional.relu(out)  \n",
        "        out = self.dropout_1(out)  \n",
        "        out = self.fc_2(out)\n",
        "        out = self.bn_2(out)  \n",
        "        out = nn.functional.relu(out)  \n",
        "        out = self.dropout_2(out) \n",
        "        out = self.fc_3(out)\n",
        "        return out\n",
        "\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.gru_1 = nn.GRU(768, 256, num_layers=1,batch_first=True)\n",
        "        self.gru_2 = nn.GRU(256, 64, num_layers=1,batch_first=True)\n",
        "        self.fc_1 = nn.Linear(64, 16)\n",
        "        self.fc_2 = nn.Linear(16, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru_1(x)  \n",
        "        out, _ = self.gru_2(out)              \n",
        "        out = self.fc_1(out[:, -1, :])\n",
        "        out = self.fc_2(out)\n",
        "        return out\n",
        "    \n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.rnn_1 = nn.RNN(768, 256, num_layers=1,batch_first=True)\n",
        "        self.rnn_2 = nn.RNN(256, 64, num_layers=1,batch_first=True)\n",
        "        self.fc_1 = nn.Linear(64, 16)\n",
        "        self.fc_2 = nn.Linear(16, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn_1(x)  \n",
        "        out, _ = self.rnn_2(out)              \n",
        "        out = self.fc_1(out[:, -1, :])\n",
        "        # out = nn.functional.relu(out)  \n",
        "        out = self.fc_2(out)\n",
        "        return out\n",
        "    \n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.lstm_1 = nn.LSTM(768, 256, num_layers=1,batch_first=True)\n",
        "        self.lstm_2 = nn.LSTM(256, 64, num_layers=1,batch_first=True)\n",
        "        self.fc_1 = nn.Linear(64, 16)\n",
        "        self.fc_2 = nn.Linear(16, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm_1(x)  \n",
        "        out, _ = self.lstm_2(out)              \n",
        "        out = self.fc_1(out[:, -1, :])\n",
        "        # out = nn.functional.relu(out)  \n",
        "        out = self.fc_2(out)\n",
        "        return out\n",
        "\n",
        "class Brain_Teaser(Dataset):\n",
        "  def __init__(self, obj):\n",
        "    self.questions = obj[0]\n",
        "    self.choices = obj[1]\n",
        "    self.labels = obj[2]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.questions)\n",
        "  \n",
        "  def __getitem__(self, id):\n",
        "    return self.questions[id], self.choices[id], self.labels[id]  \n",
        "  \n",
        "class Brain_Teaser_2(Dataset):\n",
        "  def __init__(self, tokenizer, questions, choices, labels, max_len=512):\n",
        "    self.questions = questions\n",
        "    self.choices = choices\n",
        "    self.labels = labels\n",
        "\n",
        "    self.max_len = max_len\n",
        "    self.tokenizer = tokenizer\n",
        "    self.inputs = []\n",
        "    self.targets = []\n",
        "    self.question_options_encoded = []\n",
        "\n",
        "    self.build_questions()\n",
        "  \n",
        "  def build_questions(self):\n",
        "    maxi = 0\n",
        "    for id in range(len(self.questions)):\n",
        "      qo = []\n",
        "      for option_id in range(4):\n",
        "        question_options = \"Question : \" + self.questions[id] + ' ' + \"Option: \" + str(option_id) + ' ' + self.choices[id][option_id] \n",
        "        qo.append(self.tokenizer.encode(question_options, convert_to_tensor = True))\n",
        "      self.question_options_encoded.append(qo)\n",
        "    \n",
        "  def __len__(self):\n",
        "    return len(self.question_options_encoded)\n",
        "  \n",
        "  def __getitem__(self, id):\n",
        "    return torch.stack(self.question_options_encoded[id]),  self.labels[id]  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "4t_BdYEY9Qvr"
      },
      "outputs": [],
      "source": [
        "data = np.load(\"data/SP-train.npy\", allow_pickle=True)\n",
        "\n",
        "o_data = []\n",
        "sr_data = []\n",
        "cr_data = []\n",
        "\n",
        "for i in range(len(data)):\n",
        "    size_ = len(data[i]['id'])\n",
        "\n",
        "    if data[i]['id'][size_-2:size_] == 'CR':\n",
        "        cr_data.append(data[i])\n",
        "    elif data[i]['id'][size_-2:size_] == 'SR':\n",
        "        sr_data.append(data[i])\n",
        "    else:\n",
        "        o_data.append(data[i])\n",
        "model_name=\"bert-large-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "train_data = o_data[:int(len(o_data)*0.8)] + cr_data[:int(len(cr_data)*0.8)] + sr_data[:int(len(sr_data)*0.8)]\n",
        "val_data = o_data[int(len(o_data)*0.8):int(len(o_data)*0.9)] + cr_data[int(len(cr_data)*0.8):int(len(cr_data)*0.9)] + sr_data[int(len(sr_data)*0.8):int(len(sr_data)*0.9)]\n",
        "test_data = o_data[int(len(o_data)*0.9):] + cr_data[int(len(cr_data)*0.9):] + sr_data[int(len(sr_data)*0.9):]\n",
        "\n",
        "o_test_data = o_data[int(len(o_data)*0.9):]\n",
        "c_test_data = cr_data[int(len(cr_data)*0.9):] \n",
        "s_test_data =  sr_data[int(len(sr_data)*0.9):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedding_model = SentenceTransformer('distilbert-base-nli-mean-tokens')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Y5uPu6jXk9a0"
      },
      "outputs": [],
      "source": [
        "\n",
        "o_test_dataset = Brain_Teaser(get_data_lists(o_test_data))\n",
        "c_test_dataset = Brain_Teaser(get_data_lists(c_test_data))\n",
        "s_test_dataset = Brain_Teaser(get_data_lists(s_test_data))\n",
        "\n",
        "train_data = o_data[:int(len(o_data)*0.8)] + cr_data[:int(len(cr_data)*0.8)] + sr_data[:int(len(sr_data)*0.8)]\n",
        "val_data = o_data[int(len(o_data)*0.8):int(len(o_data)*0.9)] + cr_data[int(len(cr_data)*0.8):int(len(cr_data)*0.9)] + sr_data[int(len(sr_data)*0.8):int(len(sr_data)*0.9)]\n",
        "test_data = o_data[int(len(o_data)*0.9):] + cr_data[int(len(cr_data)*0.9):] + sr_data[int(len(sr_data)*0.9):]\n",
        "\n",
        "\n",
        "train_questions, train_choices, train_labels = get_data_lists(train_data)\n",
        "val_questions, val_choices, val_labels = get_data_lists(val_data)\n",
        "test_questions, test_choices, test_labels = get_data_lists(test_data)\n",
        "\n",
        "train_dataset_2 = Brain_Teaser_2(embedding_model, train_questions, train_choices, train_labels)\n",
        "val_dataset_2 = Brain_Teaser_2(embedding_model, val_questions, val_choices, val_labels)\n",
        "test_dataset_2 = Brain_Teaser_2(embedding_model, test_questions, test_choices, test_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[-0.6462, -0.2440,  0.2941,  ..., -0.0312,  0.3269, -0.2828],\n",
              "         [-0.4100, -0.1910,  0.3373,  ...,  0.0210,  0.3945, -0.5179],\n",
              "         [-0.4757, -0.2962,  0.3105,  ...,  0.0132,  0.3523, -0.6067],\n",
              "         [-0.3288, -0.3289,  0.4456,  ..., -0.0748,  0.2734, -0.7899]],\n",
              "        device='mps:0'),\n",
              " 3)"
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset_2[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "1jPSaVR6aNET"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sentence Transfomers + Sequential Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_model(test_dataset, model, batch = 1):\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    # test_dataloader = DataLoader(test_dataset, batch_size=batch, shuffle=True)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    with torch.no_grad():\n",
        "        for id in range(len(test_dataset)):\n",
        "            inputs = test_dataset[id][0].view(1,4,768)\n",
        "            targets = test_dataset[id][1]\n",
        "            inputs = inputs.to(device)\n",
        "            # targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += 1\n",
        "            # print(predicted[0], targets)\n",
        "            correct += (predicted == targets).sum().item()\n",
        "    print(f\"Test Accuracy: {(100 * correct / total):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RNN\n",
            "Test Accuracy: 47.06%\n",
            "LSTM\n",
            "Test Accuracy: 31.37%\n",
            "GRU\n",
            "Test Accuracy: 27.45%\n"
          ]
        }
      ],
      "source": [
        "print(\"RNN\")\n",
        "model_sentence_rnn = RNNModel()\n",
        "model_sentence_rnn.load_state_dict(torch.load('trained_models/sb_rnn.pt'))\n",
        "test_model(test_dataset_2, model_sentence_rnn)\n",
        "\n",
        "print(\"LSTM\")\n",
        "model_sentence_lstm = LSTMModel()\n",
        "model_sentence_lstm.load_state_dict(torch.load('trained_models/sb_lstm.pt'))\n",
        "test_model(test_dataset_2, model_sentence_lstm)\n",
        "\n",
        "print(\"GRU\")\n",
        "model_sentence_gru = GRUModel()\n",
        "model_sentence_gru.load_state_dict(torch.load('trained_models/sb_gru.pt'))\n",
        "test_model(test_dataset_2, model_sentence_gru)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Encoder Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/nalishjain/Acad Sem 6/NLP-Project/.venv/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:550: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "model_name=\"microsoft/deberta-v3-base\"\n",
        "tokenizer_deberta = AutoTokenizer.from_pretrained(model_name)\n",
        "model_deberta = AutoModelForMultipleChoice.from_pretrained(\"trained_models/deberta_model\", ignore_mismatched_sizes=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name= \"microsoft/deberta-v3-base\"\n",
        "tokenizer_deberta_ft = AutoTokenizer.from_pretrained(model_name)\n",
        "model_deberta_ft = AutoModelForMultipleChoice.from_pretrained(\"trained_models/deberta_ft_model\", ignore_mismatched_sizes=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name=\"FacebookAI/roberta-large\"\n",
        "tokenizer_roberta = AutoTokenizer.from_pretrained(model_name)\n",
        "model_roberta = AutoModelForMultipleChoice.from_pretrained(\"trained_models/roberta_model\", ignore_mismatched_sizes=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name=\"DeepPavlov/roberta-large-winogrande\"\n",
        "tokenizer_roberta_w = AutoTokenizer.from_pretrained(model_name)\n",
        "model_roberta_w = AutoModelForMultipleChoice.from_pretrained(\"trained_models/roberta_wngrd_model\", ignore_mismatched_sizes=True).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "uhKq_j7saYI5"
      },
      "outputs": [],
      "source": [
        "def get_predictions(dataset, model, tokenizer):\n",
        "\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    \n",
        "    for id in range(len(dataset)):\n",
        "        ques = dataset[id][0]\n",
        "        choices =  dataset[id][1]\n",
        "        true_label =  dataset[id][2]\n",
        "\n",
        "        inputs = tokenizer([[ques, choices[0]], [ques, choices[1]], [ques, choices[2]], [ques, choices[3]]], return_tensors = \"pt\", padding = True).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**{key: value.unsqueeze(0) for key, value in inputs.items()})\n",
        "        logits = outputs.logits\n",
        "        predicted_class = logits.argmax().item()\n",
        "        predictions.append(predicted_class)\n",
        "        targets.append(true_label)\n",
        "    \n",
        "    return predictions, targets\n",
        "\n",
        "def ensemble_predictions(predictions):\n",
        "    num_samples = len(predictions[0])\n",
        "    ensemble_pred = []\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        counts = Counter([pred[i] for pred in predictions])\n",
        "        majority_vote = counts.most_common(1)[0][0]\n",
        "        ensemble_pred.append(majority_vote)\n",
        "\n",
        "    return ensemble_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Original Puzzles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "1do2YeYAQ4FY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deberta \n",
            "0.7647058823529411\n",
            "Deberta FT\n",
            "0.8235294117647058\n",
            "Roberta\n",
            "0.7647058823529411\n",
            "Roberta Winogrande\n",
            "0.8235294117647058\n"
          ]
        }
      ],
      "source": [
        "deberta_prediction_o, target_o = get_predictions(o_test_dataset, model_deberta, tokenizer_deberta)\n",
        "print(\"Deberta \")\n",
        "print(accuracy_score(target_o, deberta_prediction_o ))\n",
        "\n",
        "deberta_ft_prediction_o, target_o = get_predictions(o_test_dataset, model_deberta_ft, tokenizer_deberta_ft)\n",
        "print(\"Deberta FT\")\n",
        "print(accuracy_score(target_o, deberta_ft_prediction_o ))\n",
        "\n",
        "roberta_prediction_o, target_o = get_predictions(o_test_dataset, model_roberta, tokenizer_roberta)\n",
        "print(\"Roberta\")\n",
        "print(accuracy_score(target_o, roberta_prediction_o))\n",
        "\n",
        "roberta_w_prediction_o, target_o = get_predictions(o_test_dataset, model_roberta_w, tokenizer_roberta_w)\n",
        "print(\"Roberta Winogrande\")\n",
        "print(accuracy_score(target_o, roberta_w_prediction_o ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Semantically Reconstructed Puzzles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deberta \n",
            "0.7647058823529411\n",
            "Deberta FT\n",
            "0.8235294117647058\n",
            "Roberta\n",
            "0.7647058823529411\n",
            "Roberta Winogrande\n",
            "0.8235294117647058\n"
          ]
        }
      ],
      "source": [
        "deberta_prediction_s, target_s = get_predictions(s_test_dataset, model_deberta, tokenizer_deberta)\n",
        "print(\"Deberta \")\n",
        "print(accuracy_score(target_s, deberta_prediction_s ))\n",
        "\n",
        "deberta_ft_prediction_s, target_s = get_predictions(s_test_dataset, model_deberta_ft, tokenizer_deberta_ft)\n",
        "print(\"Deberta FT\")\n",
        "print(accuracy_score(target_s, deberta_ft_prediction_s ))\n",
        "\n",
        "roberta_prediction_s, target_s = get_predictions(s_test_dataset, model_roberta, tokenizer_roberta)\n",
        "print(\"Roberta\")\n",
        "print(accuracy_score(target_s, roberta_prediction_s))\n",
        "\n",
        "roberta_w_prediction_s, target_s = get_predictions(s_test_dataset, model_roberta_w, tokenizer_roberta_w)\n",
        "print(\"Roberta Winogrande\")\n",
        "print(accuracy_score(target_s, roberta_w_prediction_s ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Contextually Reconstructed Puzzles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deberta \n",
            "0.7058823529411765\n",
            "Deberta FT\n",
            "0.8235294117647058\n",
            "Roberta\n",
            "0.5882352941176471\n",
            "Roberta Winogrande\n",
            "0.7058823529411765\n"
          ]
        }
      ],
      "source": [
        "\n",
        "deberta_prediction_c, target_c = get_predictions(c_test_dataset, model_deberta, tokenizer_deberta)\n",
        "print(\"Deberta \")\n",
        "print(accuracy_score(target_c, deberta_prediction_c ))\n",
        "\n",
        "deberta_ft_prediction_c, target_c = get_predictions(c_test_dataset, model_deberta_ft, tokenizer_deberta_ft)\n",
        "print(\"Deberta FT\")\n",
        "print(accuracy_score(target_c, deberta_ft_prediction_c ))\n",
        "\n",
        "roberta_prediction_c, target_c = get_predictions(c_test_dataset, model_roberta, tokenizer_roberta)\n",
        "print(\"Roberta\")\n",
        "print(accuracy_score(target_c, roberta_prediction_c))\n",
        "\n",
        "roberta_w_prediction_c, target_c = get_predictions(c_test_dataset, model_roberta_w, tokenizer_roberta_w)\n",
        "print(\"Roberta Winogrande\")\n",
        "print(accuracy_score(target_c, roberta_w_prediction_c ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Overall Accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deberta \n",
            "0.7450980392156863\n",
            "Deberta FT\n",
            "0.8235294117647058\n",
            "Roberta\n",
            "0.7058823529411765\n",
            "Roberta Winogrande\n",
            "0.7843137254901961\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "print(\"Deberta \")\n",
        "print(accuracy_score(target_o + target_s + target_c, deberta_prediction_o + deberta_prediction_s + deberta_prediction_c ))\n",
        "\n",
        "print(\"Deberta FT\")\n",
        "print(accuracy_score(target_o + target_s + target_c, deberta_ft_prediction_o + deberta_ft_prediction_s + deberta_ft_prediction_c ))\n",
        "\n",
        "print(\"Roberta\")\n",
        "print(accuracy_score(target_o + target_s + target_c, roberta_prediction_o + roberta_prediction_s + roberta_prediction_c ))\n",
        "\n",
        "print(\"Roberta Winogrande\")\n",
        "print(accuracy_score(target_o + target_s + target_c, roberta_w_prediction_o + roberta_w_prediction_s + roberta_w_prediction_c ))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ensemble "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "prediction_s = [\n",
        "    deberta_prediction_s,\n",
        "    roberta_w_prediction_s,\n",
        "    deberta_ft_prediction_s\n",
        "]\n",
        "\n",
        "prediction_c = [\n",
        "    deberta_prediction_c,\n",
        "    roberta_w_prediction_c,\n",
        "    deberta_ft_prediction_c\n",
        "]\n",
        "\n",
        "prediction_o = [\n",
        "    deberta_prediction_o,\n",
        "    roberta_w_prediction_o,\n",
        "    deberta_ft_prediction_o\n",
        "]\n",
        "\n",
        "\n",
        "ensemble_pred_o = ensemble_predictions(prediction_o)\n",
        "ensemble_pred_s = ensemble_predictions(prediction_s)\n",
        "ensemble_pred_c = ensemble_predictions(prediction_c)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.8823529411764706\n",
            "0.8235294117647058\n",
            "0.7058823529411765\n"
          ]
        }
      ],
      "source": [
        "print(accuracy_score(target_o, ensemble_pred_o))\n",
        "print(accuracy_score(target_s, ensemble_pred_s))\n",
        "print(accuracy_score(target_c, ensemble_pred_c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "o_test_dataset_2 = Brain_Teaser(get_data_lists(o_data))\n",
        "c_test_dataset_2 = Brain_Teaser(get_data_lists(cr_data))\n",
        "s_test_dataset_2 = Brain_Teaser(get_data_lists(sr_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Semantic Puzzles\n",
            "1.0\n",
            "Context Puzzles\n",
            "0.893491124260355\n"
          ]
        }
      ],
      "source": [
        "model_deberta_ft_o = AutoModelForMultipleChoice.from_pretrained(\"trained_models/deberta_original\", ignore_mismatched_sizes=True).to(device)\n",
        "deberta_ft_prediction_s, target_s = get_predictions(s_test_dataset_2, model_deberta_ft_o, tokenizer)\n",
        "print(\"Semantic Puzzles\")\n",
        "print(accuracy_score(target_s, deberta_ft_prediction_s))\n",
        "\n",
        "deberta_ft_prediction_c, target_c = get_predictions(c_test_dataset_2, model_deberta_ft_o, tokenizer)\n",
        "print(\"Context Puzzles\")\n",
        "print(accuracy_score(target_c, deberta_ft_prediction_c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Puzzles\n",
            "0.9940828402366864\n",
            "Context Puzzles\n",
            "0.8875739644970414\n"
          ]
        }
      ],
      "source": [
        "model_deberta_ft_s = AutoModelForMultipleChoice.from_pretrained(\"trained_models/deberta_semantic\", ignore_mismatched_sizes=True).to(device)\n",
        "deberta_ft_prediction_o, target_o = get_predictions(o_test_dataset_2, model_deberta_ft_s, tokenizer)\n",
        "print(\"Original Puzzles\")\n",
        "print(accuracy_score(target_o, deberta_ft_prediction_o))\n",
        "\n",
        "deberta_ft_prediction_c, target_c = get_predictions(c_test_dataset_2, model_deberta_ft_s, tokenizer)\n",
        "print(\"Context Puzzles\")\n",
        "print(accuracy_score(target_c, deberta_ft_prediction_c))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Puzzles\n",
            "0.863905325443787\n",
            "Semantic Puzzles\n",
            "0.8579881656804734\n"
          ]
        }
      ],
      "source": [
        "model_deberta_ft_c = AutoModelForMultipleChoice.from_pretrained(\"trained_models/deberta_context\", ignore_mismatched_sizes=True).to(device)\n",
        "deberta_ft_prediction_o, target_o = get_predictions(o_test_dataset_2, model_deberta_ft_c, tokenizer)\n",
        "print(\"Original Puzzles\")\n",
        "print(accuracy_score(target_o, deberta_ft_prediction_o))\n",
        "\n",
        "deberta_ft_prediction_s, target_s = get_predictions(s_test_dataset_2, model_deberta_ft_c, tokenizer)\n",
        "print(\"Semantic Puzzles\")\n",
        "print(accuracy_score(target_s, deberta_ft_prediction_s))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
